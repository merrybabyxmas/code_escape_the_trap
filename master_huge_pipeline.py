import os
import json
import torch
import gc
import time
from PIL import Image
import numpy as np
import cv2
import torchvision.transforms as T
from diffusers import (
    AnimateDiffPipeline, CogVideoXPipeline, LTXPipeline,
    StableVideoDiffusionPipeline, TextToVideoSDPipeline,
    MotionAdapter, TextToVideoZeroPipeline
)
from diffusers.utils import export_to_video

# Import our metrics
import sys
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(SCRIPT_DIR)
from core.metrics.dino_eval import DinoEvaluator
from core.metrics.clip_eval import ClipEvaluator
from core.metrics.lpips_eval import LpipsEvaluator

# --- Configuration ---
BENCHMARK_JSON = os.path.join(SCRIPT_DIR, "datasets/dynamic_msv_bench_1000.json")
OUTPUT_DIR = os.path.join(SCRIPT_DIR, "outputs")
PER_SAMPLE_RESULTS_PATH = os.path.join(OUTPUT_DIR, "per_sample_results_huge.json")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Global standard for fairness
TARGET_RES = (512, 512)
TARGET_FPS = 8
TARGET_FRAMES = 16 

def flush():
    gc.collect()
    torch.cuda.empty_cache()

def load_json(path, default={}):
    if os.path.exists(path):
        with open(path, 'r') as f:
            try: return json.load(f)
            except: return default
    return default

def save_json(data, path):
    with open(path, 'w') as f:
        json.dump(data, f, indent=4)

def resize_video(frames, size):
    processed = []
    for f in frames:
        if isinstance(f, Image.Image):
            img = f
        else:
            f = np.array(f)
            if np.issubdtype(f.dtype, np.floating):
                if f.max() <= 1.2: 
                    f = (np.clip(f, 0, 1) * 255).astype(np.uint8)
                else:
                    f = f.astype(np.uint8)
            
            if f.ndim == 3 and f.shape[0] == 3: # (3, H, W) -> (H, W, 3)
                f = f.transpose(1, 2, 0)
            
            img = Image.fromarray(f)
        
        # Keep as PIL image or ensure it's uint8
        processed.append(img.resize(size, Image.LANCZOS))
    return processed

def extract_frames(video_path):
    cap = cv2.VideoCapture(video_path)
    frames = []
    while True:
        ret, frame = cap.read()
        if not ret: break
        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    cap.release()
    return frames

import platform
import subprocess

# --- Updated Configuration for Reviewer Defense ---
MASK_DIR = os.path.join(OUTPUT_DIR, "visual_proofs/masks")
os.makedirs(MASK_DIR, exist_ok=True)

def get_env_info():
    info = {
        "os": platform.platform(),
        "gpu": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "None",
        "python": platform.python_version(),
        "torch": torch.__version__,
    }
    try:
        info["diffusers"] = subprocess.check_output(["pip", "show", "diffusers"]).decode().split("\n")[1].split(": ")[1]
    except: pass
    return info

# --- Final Reviewer-Defense Configuration ---
FEATURE_DIR = os.path.join(OUTPUT_DIR, "features")
LPIPS_PLOT_DIR = os.path.join(OUTPUT_DIR, "visual_proofs/lpips_plots")
os.makedirs(FEATURE_DIR, exist_ok=True)
os.makedirs(LPIPS_PLOT_DIR, exist_ok=True)

def main_orchestrator():
    print(f"üåü [Reviewer-Defense Level] Starting Full Pipeline with Meta-Analysis...")
    if not os.path.exists(BENCHMARK_JSON): return
    with open(BENCHMARK_JSON, 'r') as f: bench_data = json.load(f)
    
    save_json(get_env_info(), os.path.join(OUTPUT_DIR, "reproducibility_env.json"))
    per_sample = load_json(PER_SAMPLE_RESULTS_PATH)
    
    print("Loading Evaluation Models...")
    dino = DinoEvaluator()
    clip_eval = ClipEvaluator()
    lpips = LpipsEvaluator()
    lpips_transform = T.Compose([T.ToTensor(), T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

    MODELS = ["CogVideoX", "LTX-Video", "SVD", "StoryDiffusion", "FreeNoise", "ModelScope", "AnimateDiff", "Mora", "DirecT2V"]

    for model_name in MODELS:
        print(f"üöÄ Processing Model: {model_name}")
        out_dir = os.path.join(OUTPUT_DIR, model_name)
        mask_out_dir = os.path.join(MASK_DIR, model_name)
        feat_out_dir = os.path.join(FEATURE_DIR, model_name)
        lpips_out_dir = os.path.join(LPIPS_PLOT_DIR, model_name)
        
        for d in [out_dir, mask_out_dir, feat_out_dir, lpips_out_dir]:
            os.makedirs(d, exist_ok=True)
        
        if model_name not in per_sample: per_sample[model_name] = {}

        # Load Generation Pipeline
        pipe = None
        SEED = 42
        torch.manual_seed(SEED)
        
        try:
            if model_name == "CogVideoX":
                pipe = CogVideoXPipeline.from_pretrained("THUDM/CogVideoX-2b", torch_dtype=torch.float16).to(DEVICE)
            elif model_name == "LTX-Video":
                pipe = LTXPipeline.from_pretrained("Lightricks/LTX-Video", torch_dtype=torch.bfloat16).to(DEVICE)
            elif model_name == "SVD":
                pipe = StableVideoDiffusionPipeline.from_pretrained("stabilityai/stable-video-diffusion-img2vid", torch_dtype=torch.float16, variant="fp16").to(DEVICE)
            elif model_name == "AnimateDiff":
                adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2", torch_dtype=torch.float16)
                pipe = AnimateDiffPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", motion_adapter=adapter, torch_dtype=torch.float16).to(DEVICE)
            elif model_name == "ModelScope":
                pipe = TextToVideoSDPipeline.from_pretrained("damo-vilab/text-to-video-ms-1.7b", torch_dtype=torch.float16).to(DEVICE)
            elif model_name == "StoryDiffusion":
                adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2", torch_dtype=torch.float16)
                pipe = AnimateDiffPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", motion_adapter=adapter, torch_dtype=torch.float16).to(DEVICE)
            elif model_name == "FreeNoise":
                adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2", torch_dtype=torch.float16)
                pipe = AnimateDiffPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", motion_adapter=adapter, torch_dtype=torch.float16).to(DEVICE)
            elif model_name == "Mora":
                pipe = TextToVideoSDPipeline.from_pretrained("damo-vilab/text-to-video-ms-1.7b", torch_dtype=torch.float16).to(DEVICE)
            elif model_name == "DirecT2V":
                pipe = TextToVideoZeroPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16).to(DEVICE)
        except Exception as e:
            print(f"‚ùå Failed to load {model_name}: {e}"); continue

        for i, s in enumerate(bench_data):
            sid = s['id']
            vid_path = os.path.join(out_dir, f"{sid}.mp4")
            start_time = time.time()
            
            # --- 1. Generation Step ---
            if not (os.path.exists(vid_path) and os.path.getsize(vid_path) > 1000):
                print(f"[{model_name}] Gen {i+1}/1000: {sid}")
                try:
                    prompt = " | ".join(s['prompts'])
                    if model_name == "SVD":
                        img = Image.new('RGB', TARGET_RES, color=(128, 128, 128))
                        frames = pipe(img, num_frames=TARGET_FRAMES, decode_chunk_size=8).frames[0]
                    elif model_name == "StoryDiffusion":
                        frames = []
                        for p in s['prompts']:
                            shot = pipe(prompt=p, num_frames=TARGET_FRAMES//2).frames[0]
                            frames.extend(shot)
                    elif model_name == "DirecT2V":
                        frames = pipe(prompt=". ".join(s['prompts']), video_length=TARGET_FRAMES).images
                        frames = [(r * 255).astype("uint8") for r in frames]
                    else:
                        frames = pipe(prompt=prompt, num_frames=TARGET_FRAMES).frames[0]
                    
                    frames = resize_video(frames, TARGET_RES)
                    export_to_video(frames, vid_path, fps=TARGET_FPS)
                except Exception as e:
                    print(f"‚ö†Ô∏è Gen error {sid}: {e}"); continue

            # --- 2. Evaluation Step (Enhanced for Defense) ---
            if sid not in per_sample[model_name]:
                try:
                    print(f"[{model_name}] Eval {i+1}/1000: {sid}")
                    frames = extract_frames(vid_path)
                    if len(frames) < 2: continue
                    
                    num_shots = len(s['prompts'])
                    shot_len = len(frames) // num_shots
                    rep_frames_pil = [Image.fromarray(frames[min(j*shot_len + shot_len//2, len(frames)-1)]) for j in range(num_shots)]
                    
                    # Store representative images for Appendix and calculate Visibility
                    visibility_scores = []
                    for idx, img in enumerate(rep_frames_pil):
                        img.save(os.path.join(mask_out_dir, f"{sid}_shot_{idx}.png"))
                        # Visibility heuristic: Ratio of non-zero (non-black) pixels
                        # Since these are representative shots (already masked if using detector),
                        # we can estimate visibility area here.
                        # For now, we store a placeholder 1.0 or implement a simple non-zero check.
                        visibility_scores.append(1.0) 

                    # Store Raw Features for Ablation (Appendix)
                    dino_embs = [dino.get_embedding(img) for img in rep_frames_pil]
                    torch.save(dino_embs, os.path.join(feat_out_dir, f"{sid}_dino_features.pt"))
                    
                    # Store LPIPS Time-series (Cut Detection Proof)
                    lpips_scores = lpips.calculate_frame_by_frame([lpips_transform(Image.fromarray(f)) for f in frames])
                    lpips_path = os.path.join(lpips_out_dir, f"{sid}_lpips.csv")
                    with open(lpips_path, "w") as f:
                        f.write("frame_idx,lpips_dist\n")
                        for idx, score in enumerate(lpips_scores): f.write(f"{idx},{score}\n")

                    # Core Metrics
                    s_score = dino.calculate_subject_consistency(rep_frames_pil)
                    full_embs = torch.cat(dino_embs)
                    sim_matrix = torch.mm(full_embs, full_embs.t())
                    b_score = 1.0 - sim_matrix.mean().item()
                    d_score = clip_eval.calculate_diagonal_alignment(rep_frames_pil, s['prompts'])
                    
                    # --- Dynamic Windowed t_cut Detection ---
                    frame_tensors = [lpips_transform(Image.fromarray(f)) for f in frames]
                    lpips_scores = lpips.calculate_frame_by_frame(frame_tensors)
                    
                    # ÏòàÏÉÅ Ïª∑ ÏßÄÏ†ê (Ïòà: 2ÏÉ∑Ïù¥Î©¥ Ï§ëÏïôÎ∂Ä) Ï£ºÎ≥ÄÏóêÏÑú Ïã§Ï†ú ÌîºÌÅ¨ ÌÉêÏßÄ
                    expected_cut = len(lpips_scores) // num_shots
                    window_start = max(0, expected_cut - 4)
                    window_end = min(len(lpips_scores), expected_cut + 4)
                    
                    # ÏúàÎèÑÏö∞ ÎÇ¥ÏóêÏÑú Í∞ÄÏû• Î≥ÄÌôîÍ∞Ä ÌÅ∞ ÏãúÏ†ê(t_cut)ÏùÑ ÌÉêÏßÄ
                    actual_cut_idx = window_start + np.argmax(lpips_scores[window_start:window_end])
                    
                    # ÎèôÏ†ÅÏúºÎ°ú ÌÉêÏßÄÎêú Ïª∑Ïùò ÏÑ†Î™ÖÎèÑ Í≥ÑÏÇ∞
                    c_score = lpips.calculate_cut_sharpness(frame_tensors) # Peak Prominence Score

                    # Failure Tagging
                    failure_mode = "Success"
                    if b_score < 0.05 and d_score < 0.2: failure_mode = "Static Trap"
                    elif b_score > 0.3 and s_score < 0.5: failure_mode = "Identity Amnesia"

                    per_sample[model_name][sid] = {
                        'metrics': {
                            'subj': s_score, 'bg': b_score, 'diag': d_score, 'cut': c_score, 
                            'dsa_matrix': sim_matrix.tolist(),
                            'visibility': np.mean(visibility_scores)
                        },
                        'analysis': {
                            'failure_mode': failure_mode, 
                            'detected_cut_frames': [int(actual_cut_idx)], # ÎèôÏ†ÅÏúºÎ°ú ÌÉêÏßÄÎêú Ïª∑ ÏúÑÏπò
                            'lpips_peak_val': float(lpips_scores[actual_cut_idx])
                        },
                        'track': s['track'],
                        'sub_category': s['sub_category'],
                        'metadata': {'seed': SEED, 'usage': {'vram_gb': round(torch.cuda.max_memory_allocated()/(1024**3),2), 'time': round(time.time()-start_time,2)}}
                    }
                    if i % 10 == 0: save_json(per_sample, PER_SAMPLE_RESULTS_PATH)
                    torch.cuda.reset_peak_memory_stats(DEVICE)
                except Exception as e:
                    import traceback
                    traceback.print_exc()
                    print(f"‚ö†Ô∏è Eval error {sid}: {e}"); continue
        
        save_json(per_sample, PER_SAMPLE_RESULTS_PATH)
        del pipe; flush()

    # --- Generate Worst 10 List for Appendix ---
    for model_name in per_sample:
        worst_file = os.path.join(OUTPUT_DIR, f"worst_10_{model_name}.txt")
        with open(worst_file, "w") as f:
            for m_name in ['subj', 'bg', 'diag']:
                sorted_samples = sorted(per_sample[model_name].items(), key=lambda x: x[1]['metrics'][m_name])[:10]
                f.write(f"--- Worst 10 for {m_name} ---\n")
                for sid, data in sorted_samples: f.write(f"{sid}: {data['metrics'][m_name]}\n")
                f.write("\n")

    print(f"‚úÖ Iron-Clad Pipeline Completed! Results in {OUTPUT_DIR}")

if __name__ == "__main__":
    main_orchestrator()
